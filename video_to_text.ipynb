{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91671b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/real_time_asl/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import whisper\n",
    "import os\n",
    "import tempfile\n",
    "import av  # PyAV as a modern alternative\n",
    "import numpy as np\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae04ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Translate to ASL gloss using a pre-trained translation model (MarianMT)\n",
    "asl_model_name = \"Helsinki-NLP/opus-mt-en-ROMANCE\"  # Using generic EN translator for demo\n",
    "asl_tokenizer = MarianTokenizer.from_pretrained(asl_model_name)\n",
    "asl_model = MarianMTModel.from_pretrained(asl_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb54de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract audio and generate transcript from video using Whisper\n",
    "def extract_audio_and_transcribe(video_path):\n",
    "    print(\"Extracting audio and transcribing...\")\n",
    "    temp_audio_path = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False).name\n",
    "\n",
    "    # Extract audio using FFmpeg (PyAV can't easily write raw WAV directly)\n",
    "    os.system(f\"ffmpeg -i \\\"{video_path}\\\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \\\"{temp_audio_path}\\\" -y\")\n",
    "\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(temp_audio_path)\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    # Return as list of tuples (start, end, text)\n",
    "    transcript = [(seg['start'], seg['end'], seg['text']) for seg in result['segments']]\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def asl_translate(text):\n",
    "    batch = asl_tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\")\n",
    "    translated = asl_model.generate(**batch)\n",
    "    return asl_tokenizer.decode(translated[0], skip_special_tokens=True).upper()\n",
    "\n",
    "# Step 3: Overlay ASL gloss on video\n",
    "\n",
    "def overlay_asl_on_video(input_video_path, output_video_path, transcript):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    subtitle_index = 0\n",
    "    asl_text = \"\"\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        current_time = frame_count / fps\n",
    "\n",
    "        if subtitle_index < len(transcript):\n",
    "            start, end, text = transcript[subtitle_index]\n",
    "            if start <= current_time <= end:\n",
    "                asl_text = asl_translate(text)\n",
    "            elif current_time > end:\n",
    "                subtitle_index += 1\n",
    "                asl_text = \"\"\n",
    "\n",
    "        if asl_text:\n",
    "            cv2.rectangle(frame, (50, height - 100), (width - 50, height - 30), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, asl_text, (60, height - 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"ASL subtitle video saved as:\", output_video_path)\n",
    "\n",
    "# Main pipeline\n",
    "def process_video_to_asl_subtitles(input_video_path, output_video_path):\n",
    "    transcript = extract_audio_and_transcribe(input_video_path)\n",
    "    overlay_asl_on_video(input_video_path, output_video_path, transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f40b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio and transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'input_video.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2024-10-06T05:17:49.000000Z\n",
      "    encoder         : Google\n",
      "  Duration: 00:00:42.07, start: 0.000000, bitrate: 428 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 360x640 [SAR 1:1 DAR 9:16], 297 kb/s, 30 fps, 30 tbr, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-10-06T05:17:49.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 10/05/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-10-06T05:17:49.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 10/05/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/tmp/tmphuzazpva.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf60.16.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-10-06T05:17:49.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 10/05/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc60.31.102 pcm_s16le\n",
      "[out#0/wav @ 0x5650edc4fe80] video:0kB audio:1315kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.005793%\n",
      "size=    1315kB time=00:00:42.07 bitrate= 256.0kbits/s speed= 205x    \n",
      "100%|███████████████████████████████████████| 139M/139M [00:05<00:00, 28.0MiB/s]\n",
      "/home/kevin/miniconda3/envs/real_time_asl/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4106: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL subtitle video saved as: asl_subtitled_output.mp4\n"
     ]
    }
   ],
   "source": [
    "process_video_to_asl_subtitles(\"input_video.mp4\", \"asl_subtitled_output.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_time_asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
