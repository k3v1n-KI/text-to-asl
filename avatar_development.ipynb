{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d155c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tempfile\n",
    "import whisper\n",
    "import mediapipe as mp\n",
    "\n",
    "from ASL_model import TexttoMPPoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607a3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TexttoMPPoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8afb9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract audio and generate transcript from video using Whisper\n",
    "def extract_audio_and_transcribe(video_path):\n",
    "    print(\"Extracting audio and transcribing...\")\n",
    "    temp_audio_path = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False).name\n",
    "\n",
    "    # Extract audio using FFmpeg (PyAV can't easily write raw WAV directly)\n",
    "    os.system(f\"ffmpeg -i \\\"{video_path}\\\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \\\"{temp_audio_path}\\\" -y\")\n",
    "\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(temp_audio_path)\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    # Return as list of tuples (start, end, text)\n",
    "    transcript = [(seg['start'], seg['end'], seg['text']) for seg in result['segments']]\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79bd477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) sanitize transcript words\n",
    "def sanitize_word(w):\n",
    "    w = w.lower().strip()\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", w)\n",
    "\n",
    "# 2) build per-word timings\n",
    "def build_word_timings(transcript):\n",
    "    word_times = []\n",
    "    for start, end, text in transcript:\n",
    "        raw = text.strip().split()\n",
    "        dur = end - start\n",
    "        for i, rw in enumerate(raw):\n",
    "            w = sanitize_word(rw)\n",
    "            if not w: \n",
    "                continue\n",
    "            w_start = start + (i/len(raw))*dur\n",
    "            w_end   = start + ((i+1)/len(raw))*dur\n",
    "            word_times.append({\"word\": w, \"start\": w_start, \"end\": w_end})\n",
    "    return word_times\n",
    "\n",
    "# 3) load GT JSONs\n",
    "def load_gt_landmarks(folder=\"landmark_data\"):\n",
    "    gt = {}\n",
    "    for fn in os.listdir(folder):\n",
    "        if not fn.endswith(\".json\"):\n",
    "            continue\n",
    "        w = fn[:-5]\n",
    "        gt[w] = json.load(open(os.path.join(folder, fn)))\n",
    "    return gt\n",
    "\n",
    "# 4) precompute predictions\n",
    "def load_pred_landmarks(words):\n",
    "    preds = {}\n",
    "    for raw in set(words):\n",
    "        w = sanitize_word(raw)\n",
    "        if not w: \n",
    "            continue\n",
    "        preds[w] = model.predict_landmarks(w)\n",
    "    return preds\n",
    "\n",
    "mp_pose      = mp.solutions.pose\n",
    "mp_hands     = mp.solutions.hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "POSE_CONNECTIONS     = mp_pose.POSE_CONNECTIONS\n",
    "HAND_CONNECTIONS     = mp_hands.HAND_CONNECTIONS\n",
    "FACE_CONNECTIONS     = mp_face_mesh.FACEMESH_TESSELATION\n",
    "\n",
    "def create_avatar_figure_image(flm, width=300, height=300):\n",
    "    \"\"\"\n",
    "    Build a 300×300 RGBA avatar:\n",
    "     - Pose skeleton in white\n",
    "     - Facial mesh in green\n",
    "     - Left hand in magenta\n",
    "     - Right hand in cyan\n",
    "    \"\"\"\n",
    "    im = np.zeros((height, width, 4), np.uint8)\n",
    "\n",
    "    def to_px(pt):\n",
    "        return int(pt[\"x\"]*width), int(pt[\"y\"]*height)\n",
    "\n",
    "    # draw a connection set with given color & list of points\n",
    "    def draw_connections(points, connections, color, thickness=2):\n",
    "        for a,b in connections:\n",
    "            if a < len(points) and b < len(points):\n",
    "                pa = to_px(points[a])\n",
    "                pb = to_px(points[b])\n",
    "                cv2.line(im, pa, pb, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    # 1) Pose\n",
    "    if \"pose\" in flm:\n",
    "        draw_connections(\n",
    "            flm[\"pose\"],\n",
    "            POSE_CONNECTIONS,\n",
    "            color=(255,255,255,200),\n",
    "            thickness=3\n",
    "        )\n",
    "\n",
    "    # 2) Face\n",
    "    if \"face\" in flm:\n",
    "        draw_connections(\n",
    "            flm[\"face\"],\n",
    "            FACE_CONNECTIONS,\n",
    "            color=(0,255,0,100),\n",
    "            thickness=1\n",
    "        )\n",
    "        # draw key facial landmarks (eyes, lips) a bit more boldly?\n",
    "        for idx in [33,133,362,263]:  # outer landmarks of both eyes\n",
    "            cv2.circle(im, to_px(flm[\"face\"][idx]), 2, (0,200,0,255), -1)\n",
    "\n",
    "    # 3) Left hand\n",
    "    if \"left_hand\" in flm and flm[\"left_hand\"]:\n",
    "        draw_connections(\n",
    "            flm[\"left_hand\"],\n",
    "            HAND_CONNECTIONS,\n",
    "            color=(255,0,255,200),\n",
    "            thickness=2\n",
    "        )\n",
    "        # draw wrist/joints\n",
    "        for p in flm[\"left_hand\"]:\n",
    "            cv2.circle(im, to_px(p), 2, (255,0,255,255), -1)\n",
    "\n",
    "    # 4) Right hand\n",
    "    if \"right_hand\" in flm and flm[\"right_hand\"]:\n",
    "        draw_connections(\n",
    "            flm[\"right_hand\"],\n",
    "            HAND_CONNECTIONS,\n",
    "            color=(0,255,255,200),\n",
    "            thickness=2\n",
    "        )\n",
    "        for p in flm[\"right_hand\"]:\n",
    "            cv2.circle(im, to_px(p), 2, (0,255,255,255), -1)\n",
    "\n",
    "    return im\n",
    "\n",
    "# 6) alpha-blend a small RGBA overlay into `img` at (x,y)\n",
    "def overlay_image_alpha(img, overlay, x, y):\n",
    "    h, w = overlay.shape[:2]\n",
    "    alpha = overlay[:,:,3] / 255.0\n",
    "    inv = 1.0 - alpha\n",
    "    for c in range(3):\n",
    "        img[y:y+h, x:x+w, c] = (\n",
    "            alpha * overlay[:,:,c] +\n",
    "            inv   * img[y:y+h, x:x+w, c]\n",
    "        ).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a33aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio and transcribing...\n"
     ]
    }
   ],
   "source": [
    "# paths & prep\n",
    "video_path = \"sample_text_video.mp4\"\n",
    "transcript = extract_audio_and_transcribe(video_path)\n",
    "word_times = build_word_timings(transcript)\n",
    "\n",
    "gt_land   = load_gt_landmarks(\"landmark_data\")\n",
    "pred_land = load_pred_landmarks([wt[\"word\"] for wt in word_times])\n",
    "\n",
    "cap    = cv2.VideoCapture(video_path)\n",
    "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "W, H   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out    = cv2.VideoWriter(\"comparison_with_avatar.mp4\",\n",
    "                         cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54903de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done — comparison_with_avatar.mp4 generated.\n"
     ]
    }
   ],
   "source": [
    "frame_i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    t = frame_i / fps\n",
    "    wt = next((w for w in word_times if w[\"start\"] <= t < w[\"end\"]), None)\n",
    "    if wt:\n",
    "        rel = (t - wt[\"start\"]) / (wt[\"end\"] - wt[\"start\"])\n",
    "        w   = wt[\"word\"]\n",
    "\n",
    "        gt_seq = gt_land.get(w, [])\n",
    "        pd_seq = pred_land.get(w, [])\n",
    "        if gt_seq and pd_seq:\n",
    "            i_gt = min(int(rel * len(gt_seq)), len(gt_seq)-1)\n",
    "            i_pd = min(int(rel * len(pd_seq)), len(pd_seq)-1)\n",
    "\n",
    "            ov_gt = create_avatar_figure_image(gt_seq[i_gt])\n",
    "            ov_pd = create_avatar_figure_image(pd_seq[i_pd])\n",
    "\n",
    "            overlay_image_alpha(frame, ov_gt,   0,    H-300)\n",
    "            overlay_image_alpha(frame, ov_pd,   W-300, H-300)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_i += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Done — comparison_with_avatar.mp4 generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cfb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip, CompositeAudioClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) load the overlaid silent video\n",
    "video = VideoFileClip(\"comparison_overlay.mp4\")\n",
    "\n",
    "# 2) grab the audio from the original\n",
    "audio = VideoFileClip(\"sample_text_video.mp4\").audio\n",
    "\n",
    "new_audioclip = CompositeAudioClip([audio])\n",
    "video.audio = new_audioclip\n",
    "\n",
    "video.write_videofile(\"comparison_overlay_final.mp4\")\n",
    "print(\"New video file created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_to_asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
