{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d155c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tempfile\n",
    "import whisper\n",
    "import mediapipe as mp\n",
    "\n",
    "from ASL_model import TexttoMPPoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"asl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "607a3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TexttoMPPoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8afb9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract audio and generate transcript from video using Whisper\n",
    "def extract_audio_and_transcribe(video_path):\n",
    "    print(\"Extracting audio and transcribing...\")\n",
    "    temp_audio_path = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False).name\n",
    "\n",
    "    # Extract audio using FFmpeg (PyAV can't easily write raw WAV directly)\n",
    "    os.system(f\"ffmpeg -i \\\"{video_path}\\\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \\\"{temp_audio_path}\\\" -y\")\n",
    "\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(temp_audio_path)\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    # Return as list of tuples (start, end, text)\n",
    "    transcript = [(seg['start'], seg['end'], seg['text']) for seg in result['segments']]\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5feb8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILLER_WORDS = {\n",
    "    \"a\", \"an\", \"the\",\n",
    "    \"is\", \"are\", \"am\", \"was\", \"were\", \"be\", \"being\", \"been\",\n",
    "    \"and\", \"or\", \"but\", \"so\", \"of\", \"to\", \"in\", \"for\", \"on\", \"at\", \"by\",\n",
    "    \"um\", \"uh\", \"like\", \"you\", \"know\", \"okay\", \"right\", \"i\", \"mean\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79bd477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) sanitize transcript words\n",
    "def sanitize_word(w):\n",
    "    \"\"\"Lowercase, strip non‑alphanumerics, skip fillers.\"\"\"\n",
    "    w = w.lower().strip()\n",
    "    w = re.sub(r\"[^a-z0-9]\", \"\", w)\n",
    "    if not w or w in FILLER_WORDS:\n",
    "        return \"\"\n",
    "    return w\n",
    "\n",
    "# 2) build per-word timings\n",
    "def build_word_timings(transcript):\n",
    "    word_times = []\n",
    "    for start, end, text in transcript:\n",
    "        raw = text.strip().split()\n",
    "        dur = end - start\n",
    "        for i, rw in enumerate(raw):\n",
    "            w = sanitize_word(rw)\n",
    "            if not w: \n",
    "                continue\n",
    "            w_start = start + (i/len(raw))*dur\n",
    "            w_end   = start + ((i+1)/len(raw))*dur\n",
    "            word_times.append({\"word\": w, \"start\": w_start, \"end\": w_end})\n",
    "    return word_times\n",
    "\n",
    "# 3) load GT JSONs\n",
    "def load_gt_landmarks(folder=\"landmark_data\"):\n",
    "    gt = {}\n",
    "    for fn in os.listdir(folder):\n",
    "        if not fn.endswith(\".json\"):\n",
    "            continue\n",
    "        w = fn[:-5]\n",
    "        gt[w] = json.load(open(os.path.join(folder, fn)))\n",
    "    return gt\n",
    "\n",
    "# 4) precompute predictions\n",
    "def load_pred_landmarks(words):\n",
    "    preds = {}\n",
    "    for raw in set(words):\n",
    "        w = sanitize_word(raw)\n",
    "        if not w: \n",
    "            continue\n",
    "        preds[w] = model.predict_landmarks(w)\n",
    "    return preds\n",
    "\n",
    "mp_pose      = mp.solutions.pose\n",
    "mp_hands     = mp.solutions.hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "POSE_CONNECTIONS     = mp_pose.POSE_CONNECTIONS\n",
    "HAND_CONNECTIONS     = mp_hands.HAND_CONNECTIONS\n",
    "FACE_CONNECTIONS     = mp_face_mesh.FACEMESH_TESSELATION\n",
    "\n",
    "def create_avatar_figure_image(flm, width=300, height=300):\n",
    "    \"\"\"\n",
    "    Build a 300×300 RGBA avatar:\n",
    "     - Pose skeleton in white\n",
    "     - Facial mesh in green\n",
    "     - Left hand in magenta\n",
    "     - Right hand in cyan\n",
    "    \"\"\"\n",
    "    im = np.zeros((height, width, 4), np.uint8)\n",
    "\n",
    "    def to_px(pt):\n",
    "        return int(pt[\"x\"]*width), int(pt[\"y\"]*height)\n",
    "\n",
    "    # draw a connection set with given color & list of points\n",
    "    def draw_connections(points, connections, color, thickness=2):\n",
    "        for a,b in connections:\n",
    "            if a < len(points) and b < len(points):\n",
    "                pa = to_px(points[a])\n",
    "                pb = to_px(points[b])\n",
    "                cv2.line(im, pa, pb, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    # 1) Pose\n",
    "    if \"pose\" in flm:\n",
    "        draw_connections(\n",
    "            flm[\"pose\"],\n",
    "            POSE_CONNECTIONS,\n",
    "            color=(255,255,255,200),\n",
    "            thickness=3\n",
    "        )\n",
    "\n",
    "    # 2) Face\n",
    "    if \"face\" in flm:\n",
    "        draw_connections(\n",
    "            flm[\"face\"],\n",
    "            FACE_CONNECTIONS,\n",
    "            color=(0,255,0,100),\n",
    "            thickness=1\n",
    "        )\n",
    "        # draw key facial landmarks (eyes, lips) a bit more boldly?\n",
    "        for idx in [33,133,362,263]:  # outer landmarks of both eyes\n",
    "            cv2.circle(im, to_px(flm[\"face\"][idx]), 2, (0,200,0,255), -1)\n",
    "\n",
    "    # 3) Left hand\n",
    "    if \"left_hand\" in flm and flm[\"left_hand\"]:\n",
    "        draw_connections(\n",
    "            flm[\"left_hand\"],\n",
    "            HAND_CONNECTIONS,\n",
    "            color=(255,0,255,200),\n",
    "            thickness=2\n",
    "        )\n",
    "        # draw wrist/joints\n",
    "        for p in flm[\"left_hand\"]:\n",
    "            cv2.circle(im, to_px(p), 2, (255,0,255,255), -1)\n",
    "\n",
    "    # 4) Right hand\n",
    "    if \"right_hand\" in flm and flm[\"right_hand\"]:\n",
    "        draw_connections(\n",
    "            flm[\"right_hand\"],\n",
    "            HAND_CONNECTIONS,\n",
    "            color=(0,255,255,200),\n",
    "            thickness=2\n",
    "        )\n",
    "        for p in flm[\"right_hand\"]:\n",
    "            cv2.circle(im, to_px(p), 2, (0,255,255,255), -1)\n",
    "\n",
    "    return im\n",
    "\n",
    "# 6) alpha-blend a small RGBA overlay into `img` at (x,y)\n",
    "def overlay_image_alpha(img, overlay, x, y):\n",
    "    h, w = overlay.shape[:2]\n",
    "    alpha = overlay[:,:,3] / 255.0\n",
    "    inv = 1.0 - alpha\n",
    "    for c in range(3):\n",
    "        img[y:y+h, x:x+w, c] = (\n",
    "            alpha * overlay[:,:,c] +\n",
    "            inv   * img[y:y+h, x:x+w, c]\n",
    "        ).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a33aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio and transcribing...\n"
     ]
    }
   ],
   "source": [
    "# paths & prep\n",
    "video_path = \"sample_text_video.mp4\"\n",
    "transcript = extract_audio_and_transcribe(video_path)\n",
    "word_times = build_word_timings(transcript)\n",
    "\n",
    "gt_land   = load_gt_landmarks(\"landmark_data\")\n",
    "pred_land = load_pred_landmarks([wt[\"word\"] for wt in word_times])\n",
    "\n",
    "cap    = cv2.VideoCapture(video_path)\n",
    "fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "W, H   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out    = cv2.VideoWriter(\"comparison_with_avatar.mp4\",\n",
    "                         cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54903de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done — comparison_with_avatar.mp4 generated.\n"
     ]
    }
   ],
   "source": [
    "frame_i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    t = frame_i / fps\n",
    "    wt = next((w for w in word_times if w[\"start\"] <= t < w[\"end\"]), None)\n",
    "    if wt:\n",
    "        rel = (t - wt[\"start\"]) / (wt[\"end\"] - wt[\"start\"])\n",
    "        w   = wt[\"word\"]\n",
    "\n",
    "        gt_seq = gt_land.get(w, [])\n",
    "        pd_seq = pred_land.get(w, [])\n",
    "        if gt_seq and pd_seq:\n",
    "            i_gt = min(int(rel * len(gt_seq)), len(gt_seq)-1)\n",
    "            i_pd = min(int(rel * len(pd_seq)), len(pd_seq)-1)\n",
    "\n",
    "            ov_gt = create_avatar_figure_image(gt_seq[i_gt])\n",
    "            ov_pd = create_avatar_figure_image(pd_seq[i_pd])\n",
    "\n",
    "            overlay_image_alpha(frame, ov_gt,   0,    H-300)\n",
    "            overlay_image_alpha(frame, ov_pd,   W-300, H-300)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_i += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Done — comparison_with_avatar.mp4 generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a39284",
   "metadata": {},
   "source": [
    "Comparison with Original Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2896ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "asl_caps = {}\n",
    "def get_asl_frame(word, rel):\n",
    "    \"\"\"\n",
    "    Returns a single BGR frame from asl_videos/<word>.mp4\n",
    "    at relative position `rel` in [0..1].\n",
    "    \"\"\"\n",
    "    path = os.path.join(\"asl_videos\", f\"{word}.mp4\")\n",
    "    if word not in asl_caps:\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "        cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        asl_caps[word] = (cap, cnt)\n",
    "    cap, cnt = asl_caps[word]\n",
    "    idx = min(int(rel * cnt), cnt-1)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "    ret, frm = cap.read()\n",
    "    return frm if ret else None\n",
    "\n",
    "# ——————————————————————————————————\n",
    "# 4) overlay helper\n",
    "# ——————————————————————————————————\n",
    "\n",
    "def overlay_image_alpha(img, overlay, x, y):\n",
    "    \"\"\"Alpha-blend RGBA overlay onto BGR img at (x,y).\"\"\"\n",
    "    h, w = overlay.shape[:2]\n",
    "    alpha = overlay[:,:,3:] / 255.0\n",
    "    inv = 1.0 - alpha\n",
    "    for c in range(3):\n",
    "        img[y:y+h, x:x+w, c] = (\n",
    "            alpha[:,:,0]*overlay[:,:,c] +\n",
    "            inv[:,:,0]*img[y:y+h, x:x+w, c]\n",
    "        ).astype(np.uint8)\n",
    "\n",
    "# ——————————————————————————————————\n",
    "# 5) draw a bigger, bolder stickman (GT)\n",
    "# ——————————————————————————————————\n",
    "\n",
    "mp_pose      = mp.solutions.pose\n",
    "mp_hands     = mp.solutions.hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "POSE_CONNS = mp_pose.POSE_CONNECTIONS\n",
    "HAND_CONNS = mp_hands.HAND_CONNECTIONS\n",
    "FACE_CONNS = mp_face_mesh.FACEMESH_TESSELATION\n",
    "\n",
    "def create_bold_stickman(flm, width=350, height=350):\n",
    "    \"\"\"\n",
    "    RGBA canvas showing:\n",
    "     - Pose skeleton in thick white\n",
    "     - Facial mesh in bold green\n",
    "     - Hands in thick magenta/cyan\n",
    "    \"\"\"\n",
    "    im = np.zeros((height, width, 4), np.uint8)\n",
    "    def P(idx_list, idx):\n",
    "        pt = idx_list[idx]\n",
    "        return int(pt[\"x\"]*width), int(pt[\"y\"]*height)\n",
    "\n",
    "    # draw connections\n",
    "    def draw(conns, pts, color, thick):\n",
    "        for a,b in conns:\n",
    "            if a < len(pts) and b < len(pts):\n",
    "                cv2.line(im, P(pts, a), P(pts, b), color, thick, cv2.LINE_AA)\n",
    "\n",
    "    # 1) Pose\n",
    "    if \"pose\" in flm:\n",
    "        draw(POSE_CONNS, flm[\"pose\"], (255,255,255,230), thick=6)\n",
    "\n",
    "    # 2) Face\n",
    "    if \"face\" in flm:\n",
    "        draw(FACE_CONNS, flm[\"face\"], (0,255,0,180), thick=2)\n",
    "        # highlight eyes/lips\n",
    "        for i in (33,133,362,263):\n",
    "            cv2.circle(im, P(flm[\"face\"],i), 4, (0,200,0,255), -1)\n",
    "\n",
    "    # 3) Left hand\n",
    "    if flm.get(\"left_hand\"):\n",
    "        draw(HAND_CONNS, flm[\"left_hand\"], (255,0,255,220), thick=4)\n",
    "        for p in flm[\"left_hand\"]:\n",
    "            cv2.circle(im, (int(p[\"x\"]*width), int(p[\"y\"]*height)), 4, (255,0,255,255), -1)\n",
    "\n",
    "    # 4) Right hand\n",
    "    if flm.get(\"right_hand\"):\n",
    "        draw(HAND_CONNS, flm[\"right_hand\"], (0,255,255,220), thick=4)\n",
    "        for p in flm[\"right_hand\"]:\n",
    "            cv2.circle(im, (int(p[\"x\"]*width), int(p[\"y\"]*height)), 4, (0,255,255,255), -1)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e752f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio and transcribing...\n"
     ]
    }
   ],
   "source": [
    "video_path = \"Video_1.mp4\"\n",
    "transcript = extract_audio_and_transcribe(video_path)\n",
    "word_times = build_word_timings(transcript)\n",
    "gt_land    = load_gt_landmarks(\"landmark_data\")\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "W, H = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(\n",
    "    \"Video_1_overlay_silent.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    fps,\n",
    "    (W,H)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fa2afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done → Video_1_overlay_silent.mp4\n"
     ]
    }
   ],
   "source": [
    "frame_i = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    t = frame_i / fps\n",
    "    wt = next((w for w in word_times if w[\"start\"] <= t < w[\"end\"]), None)\n",
    "    if wt:\n",
    "        rel = (t - wt[\"start\"]) / (wt[\"end\"] - wt[\"start\"])\n",
    "        w   = wt[\"word\"]\n",
    "\n",
    "        # 1) draw GT stickman\n",
    "        seq = gt_land.get(w, [])\n",
    "        if seq:\n",
    "            idx = min(int(rel * len(seq)), len(seq)-1)\n",
    "            ov = create_bold_stickman(seq[idx])\n",
    "            overlay_image_alpha(frame, ov, 0, H - ov.shape[0])\n",
    "\n",
    "        # 2) paste ASL video frame\n",
    "        asl_fr = get_asl_frame(w, rel)\n",
    "        if asl_fr is not None:\n",
    "            small = cv2.resize(asl_fr, (350,350), interpolation=cv2.INTER_AREA)\n",
    "            frame[H-350:H, W-350:W] = small\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_i += 1\n",
    "\n",
    "# cleanup\n",
    "cap.release()\n",
    "for cap_asl, _ in asl_caps.values():\n",
    "    cap_asl.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Done → Video_1_overlay_silent.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52cfb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import restore_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e09b325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2mp41', 'encoder': 'Lavf58.76.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1920, 1080], 'bitrate': 19350, 'fps': 29.93, 'codec_name': 'mpeg4', 'profile': '(Simple Profile)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 28.63, 'bitrate': 19352, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'mpeg4', 'video_profile': '(Simple Profile)', 'video_size': [1920, 1080], 'video_bitrate': 19350, 'video_fps': 29.93, 'video_duration': 28.63, 'video_n_frames': 856}\n",
      "c:\\Users\\PC\\anaconda3\\envs\\text_to_asl\\lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i Video_1_overlay_silent.mp4 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'isommp42', 'creation_time': '2025-05-02T03:57:19.000000Z', 'location': '+43.6472-079.7424/', 'location-eng': '+43.6472-079.7424/', 'com.android.version': '15', 'com.android.capture.fps': '30.000000', 'com.samsung.android.utc_offset': '-0400'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': 'eng', 'default': True, 'size': [1920, 1080], 'bitrate': 14422, 'fps': 29.93, 'codec_name': 'hevc', 'profile': '(Main)', 'metadata': {'Metadata': '', 'creation_time': '2025-05-02T03:57:19.000000Z', 'handler_name': 'VideoHandle', 'vendor_id': '[0][0][0][0]', 'Side data': '', 'displaymatrix': 'rotation of 90.00 degrees'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': 'eng', 'default': True, 'fps': 48000, 'bitrate': 256, 'metadata': {'Metadata': '', 'creation_time': '2025-05-02T03:57:19.000000Z', 'handler_name': 'SoundHandle', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 28.63, 'bitrate': 14682, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'hevc', 'video_profile': '(Main)', 'video_size': [1920, 1080], 'video_bitrate': 14422, 'video_fps': 29.93, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 48000, 'audio_bitrate': 256, 'video_duration': 28.63, 'video_n_frames': 856}\n",
      "c:\\Users\\PC\\anaconda3\\envs\\text_to_asl\\lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i Video_1.mp4 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Building video Video_1_final.mp4.\n",
      "MoviePy - Writing audio in Video_1_finalTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video Video_1_final.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready Video_1_final.mp4\n",
      "New video file created!\n"
     ]
    }
   ],
   "source": [
    "restore_audio(\"Video_1_overlay_silent.mp4\", \"Video_1.mp4\", \"Video_1_final.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_to_asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
