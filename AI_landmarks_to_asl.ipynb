{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d135c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tempfile\n",
    "import whisper\n",
    "\n",
    "from ASL_model import TexttoMPPoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbfd2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TexttoMPPoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e5ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract audio and generate transcript from video using Whisper\n",
    "def extract_audio_and_transcribe(video_path):\n",
    "    print(\"Extracting audio and transcribing...\")\n",
    "    temp_audio_path = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False).name\n",
    "\n",
    "    # Extract audio using FFmpeg (PyAV can't easily write raw WAV directly)\n",
    "    os.system(f\"ffmpeg -i \\\"{video_path}\\\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \\\"{temp_audio_path}\\\" -y\")\n",
    "\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(temp_audio_path)\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    # Return as list of tuples (start, end, text)\n",
    "    transcript = [(seg['start'], seg['end'], seg['text']) for seg in result['segments']]\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6d440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up transcript words so they match JSON filenames\n",
    "def sanitize_word(w):\n",
    "    # lowercase\n",
    "    w = w.lower()\n",
    "    # keep only letters+digits\n",
    "    w = re.sub(r\"[^a-z0-9]\", \"\", w)\n",
    "    return w\n",
    "\n",
    "# Build word‐level timings from Whisper segments\n",
    "def build_word_timings(transcript):\n",
    "    word_timings = []\n",
    "    for start, end, text in transcript:\n",
    "        # split on whitespace\n",
    "        raw_words = text.strip().split()\n",
    "        dur = end - start\n",
    "        for i, rw in enumerate(raw_words):\n",
    "            w = sanitize_word(rw)\n",
    "            if not w:\n",
    "                continue\n",
    "            w_start = start + (i/len(raw_words))*dur\n",
    "            w_end   = start + ((i+1)/len(raw_words))*dur\n",
    "            word_timings.append({\"word\": w, \"start\": w_start, \"end\": w_end})\n",
    "    return word_timings\n",
    "\n",
    "\n",
    "def load_gt_landmarks(folder=\"landmark_data\"):\n",
    "    gt = {}\n",
    "    for fname in os.listdir(folder):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "        word = fname[:-5]  # “about.json” → “about”\n",
    "        gt[word] = json.load(open(os.path.join(folder, fname)))\n",
    "    return gt\n",
    "\n",
    "def load_pred_landmarks_for_words(words):\n",
    "    preds = {}\n",
    "    for raw in set(words):\n",
    "        w = sanitize_word(raw)\n",
    "        if not w:\n",
    "            continue\n",
    "        preds[w]  = model.predict_landmarks(w)   \n",
    "    return preds\n",
    "\n",
    "def create_stick_figure_image(frame_landmarks, width=300, height=300):\n",
    "    img = np.zeros((height, width, 3), np.uint8)\n",
    "    def draw(pts, color):\n",
    "        for p in pts:\n",
    "            if isinstance(p, dict):\n",
    "                x,y,_ = p[\"x\"], p[\"y\"], p[\"z\"]\n",
    "            else:\n",
    "                x,y,_ = p\n",
    "            px, py = int(x*width), int(y*height)\n",
    "            cv2.circle(img, (px,py), 3, color, -1)\n",
    "\n",
    "    if \"face\"      in frame_landmarks: draw(frame_landmarks[\"face\"],      (0,255,0))\n",
    "    if \"pose\"      in frame_landmarks: draw(frame_landmarks[\"pose\"],      (255,255,255))\n",
    "    if \"left_hand\" in frame_landmarks: draw(frame_landmarks[\"left_hand\"], (255,0,255))\n",
    "    if \"right_hand\" in frame_landmarks:draw(frame_landmarks[\"right_hand\"],(0,255,255))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a87f6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio and transcribing...\n"
     ]
    }
   ],
   "source": [
    "# Open the input video\n",
    "video_path = \"sample_text_video.mp4\" \n",
    "transcript = extract_audio_and_transcribe(video_path)\n",
    "word_times  = build_word_timings(transcript)\n",
    "\n",
    "gt_landmarks   = load_gt_landmarks(\"landmark_data\")\n",
    "pred_landmarks = load_pred_landmarks_for_words([wt[\"word\"] for wt in word_times])\n",
    "\n",
    "cap     = cv2.VideoCapture(video_path)\n",
    "fps     = cap.get(cv2.CAP_PROP_FPS)\n",
    "W, H    = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out     = cv2.VideoWriter(\"comparison_overlay.mp4\",\n",
    "                          cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "207708ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Comparison video → comparison_overlay.mp4\n"
     ]
    }
   ],
   "source": [
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    t = frame_idx / fps\n",
    "\n",
    "    # 5) Find which word covers this timestamp\n",
    "    wt = next((wt for wt in word_times if wt[\"start\"] <= t < wt[\"end\"]), None)\n",
    "    if wt:\n",
    "        w = wt[\"word\"]\n",
    "        rel = (t - wt[\"start\"]) / (wt[\"end\"] - wt[\"start\"])\n",
    "\n",
    "        # lookup ground‑truth & prediction sequences\n",
    "        gt_seq   = gt_landmarks.get(w, [])\n",
    "        pred_seq = pred_landmarks.get(w, [])\n",
    "\n",
    "        # pick frame index in each\n",
    "        idx_gt   = min(int(rel * len(gt_seq)),   max(len(gt_seq)-1,0))\n",
    "        idx_pred = min(int(rel * len(pred_seq)), max(len(pred_seq)-1,0))\n",
    "\n",
    "        # draw mini‑figures\n",
    "        fig_gt   = create_stick_figure_image(gt_seq[idx_gt])\n",
    "        fig_pred = create_stick_figure_image(pred_seq[idx_pred])\n",
    "\n",
    "        # overlay bottom‑left & bottom‑right\n",
    "        frame[H-300:H, 0:300]       = fig_gt\n",
    "        frame[H-300:H, W-300:W]     = fig_pred\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Done! Comparison video → comparison_overlay.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7d8e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip, CompositeAudioClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00bb3611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2mp41', 'encoder': 'Lavf58.76.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1920, 1080], 'bitrate': 18058, 'fps': 29.91, 'codec_name': 'mpeg4', 'profile': '(Simple Profile)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 6.38, 'bitrate': 18060, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'mpeg4', 'video_profile': '(Simple Profile)', 'video_size': [1920, 1080], 'video_bitrate': 18058, 'video_fps': 29.91, 'video_duration': 6.38, 'video_n_frames': 190}\n",
      "c:\\Users\\PC\\anaconda3\\envs\\text_to_asl\\lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i comparison_overlay.mp4 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'isommp42', 'creation_time': '2025-04-20T15:58:05.000000Z', 'location': '+43.6472-079.7424/', 'location-eng': '+43.6472-079.7424/', 'com.android.version': '14', 'com.android.capture.fps': '30.000000'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': 'eng', 'default': True, 'size': [1920, 1080], 'bitrate': 14491, 'fps': 29.91, 'codec_name': 'hevc', 'profile': '(Main)', 'metadata': {'Metadata': '', 'creation_time': '2025-04-20T15:58:05.000000Z', 'handler_name': 'VideoHandle', 'vendor_id': '[0][0][0][0]', 'Side data': '', 'displaymatrix': 'rotation of 90.00 degrees'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': 'eng', 'default': True, 'fps': 48000, 'bitrate': 256, 'metadata': {'Metadata': '', 'creation_time': '2025-04-20T15:58:05.000000Z', 'handler_name': 'SoundHandle', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 6.39, 'bitrate': 14745, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'hevc', 'video_profile': '(Main)', 'video_size': [1920, 1080], 'video_bitrate': 14491, 'video_fps': 29.91, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 48000, 'audio_bitrate': 256, 'video_duration': 6.39, 'video_n_frames': 191}\n",
      "c:\\Users\\PC\\anaconda3\\envs\\text_to_asl\\lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i sample_text_video.mp4 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Building video comparison_overlay_final.mp4.\n",
      "MoviePy - Writing audio in comparison_overlay_finalTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video comparison_overlay_final.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready comparison_overlay_final.mp4\n",
      "New video file created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1) load the overlaid silent video\n",
    "video = VideoFileClip(\"comparison_overlay.mp4\")\n",
    "\n",
    "# 2) grab the audio from the original\n",
    "audio = VideoFileClip(\"sample_text_video.mp4\").audio\n",
    "\n",
    "new_audioclip = CompositeAudioClip([audio])\n",
    "video.audio = new_audioclip\n",
    "\n",
    "video.write_videofile(\"comparison_overlay_final.mp4\")\n",
    "print(\"New video file created!\")\n",
    "\n",
    "# # 3) set it on the overlaid clip and write out\n",
    "# final = video.set_audio(audio)\n",
    "# final.write_videofile(\n",
    "#     \"comparison_with_audio.mp4\",\n",
    "#     codec=\"libx264\",        # or \"mpeg4\"\n",
    "#     audio_codec=\"aac\",      # or \"libmp3lame\"\n",
    "#     fps=video.fps\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_to_asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
